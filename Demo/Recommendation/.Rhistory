powershop <- do.call("rbind", lapply(powershop, as.data.frame))
AEMO_MEDIA <- do.call("rbind", lapply(AEMO_MEDIA, as.data.frame))
lumo_energy <- do.call("rbind", lapply(lumo_energy, as.data.frame))
alintanergy <- do.call("rbind", lapply(alintanergy, as.data.frame))
simplyenergy <- do.call("rbind", lapply(simplyenergy, as.data.frame))
energy <- do.call("rbind", lapply(energy, as.data.frame))
LNG <- do.call("rbind", lapply(LNG, as.data.frame))
electricity <- do.call("rbind", lapply(electricity, as.data.frame))
gas <- do.call("rbind", lapply(gas, as.data.frame))
oil <- do.call("rbind", lapply(oil, as.data.frame))
meter <- do.call("rbind", lapply(meter, as.data.frame))
lightsout <- do.call("rbind", lapply(lightsout, as.data.frame))
power <- do.call("rbind", lapply(power, as.data.frame))
powerout <- do.call("rbind", lapply(powerout, as.data.frame))
blackout <- do.call("rbind", lapply(blackout, as.data.frame))
energybill <- do.call("rbind", lapply(energybill, as.data.frame))
billshock <- do.call("rbind", lapply(billshock, as.data.frame))
solar <- do.call("rbind", lapply(solar, as.data.frame))
battery <- do.call("rbind", lapply(battery, as.data.frame))
tesla <- do.call("rbind", lapply(tesla, as.data.frame))
energyefficiency <- do.call("rbind", lapply(energyefficiency, as.data.frame))
EAdata <- rbind(EnergyAustralia, OriginEnergy, aglenergy, powershop, AEMO_MEDIA, lumo_energy, alintanergy, simplyenergy, energy, LNG, electricity, gas, oil, meter, lightsout, power, powerout, blackout, energybill, billshock, solar, battery, tesla, energyefficiency)
write.csv(EAdata, file = "C:/Users/skumarravindran/Desktop/Sentiment Analysis/EA/data/EAdata_mel_27-05-2016.csv")
EATweets <- userTimeline('energyaustralia', n=2000, since=dat)
OETweets <- userTimeline('originenergy', n=2000, since=dat)
AGLTweets <- userTimeline('aglenergy', n=2000, since=dat)
EATweets <- do.call("rbind", lapply(EATweets, as.data.frame))
OETweets <- do.call("rbind", lapply(OETweets, as.data.frame))
AGLTweets <- do.call("rbind", lapply(AGLTweets, as.data.frame))
write.csv(ComTweetData, file = "C:/Users/skumarravindran/Desktop/Sentiment Analysis/EA/data/CompTweets_27-05-2016.csv")
melbourneGeoCode = '-33.867191,151.206896,150mi'
dat <- '2016-05-12'
EnergyAustralia = searchTwitter("@EnergyAustralia", n = 2000, since=dat)
OriginEnergy = searchTwitter("@OriginEnergy", n = 2000, since=dat)
aglenergy = searchTwitter("@aglenergy", n = 2000, since=dat)
powershop = searchTwitter("@powershop", n = 2000, since=dat)
AEMO_MEDIA = searchTwitter("@AEMO_MEDIA", n = 2000, since=dat)
energy = searchTwitter("#energy", n = 2000, since=dat, lang="en", geocode = melbourneGeoCode,retryOnRateLimit=10)
LNG = searchTwitter("#LNG", n = 2000, since=dat, lang="en", geocode = melbourneGeoCode,retryOnRateLimit=10)
electricity = searchTwitter("#electricity", n = 2000, since=dat, lang="en", geocode = melbourneGeoCode,retryOnRateLimit=10)
gas = searchTwitter("#gas", n = 2000, since=dat, lang="en", geocode = melbourneGeoCode,retryOnRateLimit=10)
oil = searchTwitter("#oil", n = 2000, since=dat, lang="en", geocode = melbourneGeoCode,retryOnRateLimit=10)
meter = searchTwitter("#meter", n = 2000, since=dat, lang="en", geocode = melbourneGeoCode,retryOnRateLimit=10)
lightsout = searchTwitter("#lightsout", n = 2000, since=dat, lang="en", geocode = melbourneGeoCode,retryOnRateLimit=10)
power = searchTwitter("#power", n = 2000, since=dat, lang="en", geocode = melbourneGeoCode,retryOnRateLimit=10)
powerout = searchTwitter("#powerout", n = 2000, since=dat, lang="en", geocode = melbourneGeoCode,retryOnRateLimit=10)
blackout = searchTwitter("#blackout", n = 2000, since=dat, lang="en", geocode = melbourneGeoCode,retryOnRateLimit=10)
energybill = searchTwitter("#energybill", n = 2000, since=dat, lang="en", geocode = melbourneGeoCode,retryOnRateLimit=10)
billshock = searchTwitter("#billshock", n = 2000, since=dat, lang="en", geocode = melbourneGeoCode,retryOnRateLimit=10)
solar = searchTwitter("#solar", n = 2000, since=dat, lang="en", geocode = melbourneGeoCode,retryOnRateLimit=10)
battery = searchTwitter("#battery", n = 2000, since=dat, lang="en", geocode = melbourneGeoCode,retryOnRateLimit=10)
tesla = searchTwitter("#tesla", n = 2000, since=dat, lang="en", geocode = melbourneGeoCode,retryOnRateLimit=10)
energyefficiency = searchTwitter("#energyefficiency", n = 2000, since=dat, lang="en", geocode = melbourneGeoCode,retryOnRateLimit=10)
# Converting to data frame
energy <- do.call("rbind", lapply(energy, as.data.frame))
LNG <- do.call("rbind", lapply(LNG, as.data.frame))
electricity <- do.call("rbind", lapply(electricity, as.data.frame))
gas <- do.call("rbind", lapply(gas, as.data.frame))
oil <- do.call("rbind", lapply(oil, as.data.frame))
meter <- do.call("rbind", lapply(meter, as.data.frame))
lightsout <- do.call("rbind", lapply(lightsout, as.data.frame))
power <- do.call("rbind", lapply(power, as.data.frame))
powerout <- do.call("rbind", lapply(powerout, as.data.frame))
blackout <- do.call("rbind", lapply(blackout, as.data.frame))
energybill <- do.call("rbind", lapply(energybill, as.data.frame))
billshock <- do.call("rbind", lapply(billshock, as.data.frame))
solar <- do.call("rbind", lapply(solar, as.data.frame))
battery <- do.call("rbind", lapply(battery, as.data.frame))
tesla <- do.call("rbind", lapply(tesla, as.data.frame))
energyefficiency <- do.call("rbind", lapply(energyefficiency, as.data.frame))
EAdata <- rbind(energy, LNG, electricity, gas, oil, meter, lightsout, power, powerout, blackout, energybill, billshock, solar, battery, tesla, energyefficiency)
write.csv(EAdata, file = "C:/Users/skumarravindran/Desktop/Sentiment Analysis/EA/data/EAdata_syd_27-05-2016.csv")
library(sentiment)
?sentiment
?stringr
library(devtools)
install_github("pablobarbera/instaR/instaR")
library(devtools)
install_github("pablobarbera/instaR/instaR")
devtools::install_github("pablobarbera/instaR/instaR")
install.packages("rvest")
library(rvest)
library(stringr)
install.packages("ggmap")
library(ggmap)
leaflet
install.packages("leaflet")
install.packages("TSP")
library(leaflet)
library(TSP)
url_stable <- 'http://www.natgeotraveller.in/magazine/section/short-breaks/'
pages <- c('','page2','page3','page4','page5','page6')
xpath_place_stable_1 <- '//*[@id="body"]/div/div'
xpath_place_stable_2 <- '/div[2]/h4'
xpath_tag_stable_1 <- '//*[@id="body"]/div/div'
xpath_tag_stable_2 <- '/div[2]/div[1]/p'
xpath_link_stable_1 <- '//*[@id="body"]/div/div'
xpath_link_stable_2 <-  '/div[2]/h4/a'
place <- data.frame()
tag <- data.frame()
link <- data.frame()
for(i in 1:6)
{
url <- paste0(url_stable,pages[i])
for(j in 2:11)
{
xpath_place = paste0(xpath_place_stable_1,'[',j,']',xpath_place_stable_2)
xpath_tag = paste0(xpath_tag_stable_1,'[',j,']',xpath_tag_stable_2)
xpath_link = paste0(xpath_link_stable_1,'[',j,']',xpath_link_stable_2)
data_place <- url %>% html() %>% html_nodes(xpath = xpath_place ) %>% html_text()
data_tag <- url %>% html() %>% html_nodes(xpath = xpath_tag ) %>% html_text()
data_link <- url %>% html() %>% html_nodes(xpath = xpath_link ) %>% html_attrs()
place <- c(place,data_place)
tag <- c(tag,data_tag)
link <- c(link,paste0('http://www.natgeotraveller.in',data_link))
}
}
place_df <- data.frame(unlist(place))
tag_df <- data.frame(unlist(tag))
link_df <- data.frame(unlist(link))
link_df <- data.frame(link_df[str_length(link_df$unlist.link.) > 29,])
short_breaks_df <- data.frame(place_df,tag_df,link_df)
names(short_breaks_df)[] <- c('Place','Tag','Link')
short_breaks_df$id <- 1:nrow(short_breaks_df)
coord_master <- data.frame()
for(i in 1:nrow(short_breaks_df)){
coord <- as.numeric(geocode(as.character(short_breaks_df[i,1]),source = 'google'))
coord_master <- rbind(coord_master,coord)
}
coord_master[is.na(coord_master)] <- 0
names(coord_master)[] <- c('lon','lat')
coord_master$id <- 1:nrow(coord_master)
coord_master <- coord_master[(coord_master$lon >= 68 & coord_master$lon <= 94) & (coord_master$lat >= 8 & coord_master$lat <= 38),]
#Joining with the short_breaks dataset
short_breaks_df <- left_join(short_breaks_df,coord_master,by=NULL)
short_breaks_df
library(dplyr)
short_breaks_df <- left_join(short_breaks_df,coord_master,by=NULL)
short_breaks_df
coord_master
data <- read.csv('C:/Users/skumarravindran/Desktop/Sentiment Analysis/EA/data/EAdata11-05-2016.csv')
samp <- data$created
head(samp)
samp = strptime(samp, format="%d/%m/%Y %H:%M")
my.lt = as.POSIXlt(samp)
new.lt = my.lt + 36000
head(new.lt)
data$created <- new.lt
ncol(data)
class(data)
class(data[2,1])
head(data[2,1])
install.packages("geosphere")
install.packages("geosphere")
library(geosphere)
library(geosphere)
library(geosphere)
install.packages("geosphere")
library(geosphere)
distm (c(-37.82262, 144.968896), c(-37.819984, 144.983449), fun = distHaversine)
distm (c(144.968896, -37.82262), c(144.983449, -37.819984), fun = distHaversine)
?distm
distm (c(144.968896, -37.82262), c(145.147773, -38.509011), fun = distHaversine)
require(devtools)
install_github("JanMultmeier/GeoData/GeoDataPackage")
library(GeoData)
install_github("JanMultmeier/GeoData/GeoDataPackage")
library(XML)
library(RCurl)
latlon2ft <- function(origin,destination){
xml.url <- paste0('http://maps.googleapis.com/maps/api/distancematrix/xml?origins=',origin,'&destinations=',destination,'&mode=driving&sensor=false')
xmlfile <- xmlParse(getURL(xml.url))
dist <- xmlValue(xmlChildren(xpathApply(xmlfile,"//distance")[[1]])$value)
distance <- as.numeric(sub(" km","",dist))
ft <- distance*3.28084 # FROM METER TO FEET
return(ft)
}
latlon2ft(origin='37.193489,-121.07395',destination='37.151616,-121.046586')
library(ggmap)
(wh <- as.numeric(geocode("the white house, dc")))
(lm <- as.numeric(geocode("lincoln memorial washington dc")))
mapdist(wh, lm, mode = "walking")
distQueryCheck()
mapdist('37.193489,-121.07395', '37.151616,-121.046586', mode = "walking")
mapdist('37.193489,-121.07395', '37.151616,-121.046586', mode = "driving")
wh
mapdist('-121.07395, 37.193489', '-121.046586, 37.151616', mode = "driving")
(wh <- as.numeric(geocode("the white house, dc")))
(lm <- as.numeric(geocode("lincoln memorial washington dc")))
(wh <- as.numeric(geocode("NGV International")))
(lm <- as.numeric(geocode("Phillip Island Nature Parks")))
(wh <- as.numeric(geocode("NGV International, melbourne")))
mapdist(wh, lm, mode = "walking")
mapdist(wh, lm, mode = "driving")
(wh <- as.numeric(geocode("NGV International, melbourne")))
(lm <- as.numeric(geocode("Phillip Island Nature Parks")))
wh
(lm <- as.numeric(geocode("Phillip Island Nature Parks")))
mapdist(144.96328 -37.81411, 145.34131 -38.51253, mode = "driving")
mapdist('144.96328 -37.81411', '145.34131 -38.51253', mode = "driving")
mapdist('144.96328, -37.81411', '145.34131, -38.51253', mode = "driving")
mapdist("144.96328, -37.81411", "145.34131, -38.51253", mode = "driving")
(wh <- as.numeric(geocode("NGV International, Melbourne")))
(lm <- as.numeric(geocode("MCG, Melbourne")))
mapdist(wh, lm, mode = "driving")
(wh <- as.numeric(geocode("NGV International, Melbourne")))
(lm <- as.numeric(geocode("MCG, Melbourne")))
mapdist(wh, lm, mode = "driving")
(wh <- as.numeric(geocode("National Gallery of Victoria, 180 Saint Kilda Road, Melbourne VIC 3006")))
(lm <- as.numeric(geocode("Melbourne Zoo")))
(wh <- as.numeric(geocode("National Gallery of Victoria, Melbourne")))
(lm <- as.numeric(geocode("Melbourne Zoo")))
mapdist(wh, lm, mode = "driving")
distQueryCheck()
latlon2ft(origin='37.193489,-121.07395',destination='37.151616,-121.046586')
(wh <- as.numeric(geocode("National Gallery of Victoria, 180 Saint Kilda Road, Melbourne VIC 3006")))
(lm <- as.numeric(geocode("Melbourne Zoo")))
mapdist(wh, lm, mode = "driving")
latlon2ft(origin='144.9686, -37.8227',destination='144.95155, -37.78413')
?mapdist
results=gmapsdistance("Washington+DC","New+York+City+NY","driving","AIzaSyCsydPILhMrRUehNBI-qRHpSomouaGa8go")
library(gmapsdistance)
install_packages("gmapsdistance")
install_packages("gmapsdistance")
install.packages("gmapsdistance")
library(gmapsdistance)
results=gmapsdistance("Washington+DC","New+York+City+NY","driving","AIzaSyCsydPILhMrRUehNBI-qRHpSomouaGa8go")
gmapsdistance("Washington+DC","New+York+City+NY","driving","AIzaSyCsydPILhMrRUehNBI-qRHpSomouaGa8go")
library(sentiment)
?classify_polarity
library(RTextTools)
library(e1071)
pos_tweets =  rbind(
c('I love this car', 'positive'),
c('This view is amazing', 'positive'),
c('I feel great this morning', 'positive'),
c('I am so excited about the concert', 'positive'),
c('He is my best friend', 'positive')
)
neg_tweets = rbind(
c('I do not like this car', 'negative'),
c('This view is horrible', 'negative'),
c('I feel tired this morning', 'negative'),
c('I am not looking forward to the concert', 'negative'),
c('He is my enemy', 'negative')
)
test_tweets = rbind(
c('feel happy this morning', 'positive'),
c('larry friend', 'positive'),
c('not like that man', 'negative'),
c('house not great', 'negative'),
c('your song annoying', 'negative')
)
tweets = rbind(pos_tweets, neg_tweets, test_tweets)
matrix= create_matrix(tweets[,1], language="english",
removeStopwords=FALSE, removeNumbers=TRUE,
stemWords=FALSE)
data <- read.csv('C:/Users/skumarravindran/Desktop/Sentiment Analysis/EA/data/test.csv')
<
matrix <- create_matrix(cbind(as.vector(data$clean_tweet)), language="english", removeNumbers=TRUE, stemWords=TRUE)
library(RTextTools)
library(topicmodels)
rowTotals <- apply(matrix , 1, sum) #Find the sum of words in each Document
matrix.new   <- matrix[rowTotals> 0, ]           #remove all docs without words
lda <- LDA(matrix.new, 5)
topics <- terms(lda)
topics <- as.data.frame(topics)
head(topics, 50)
topics(lda)
?LDA
head(topics, 50)
setwd("C:\\Users\\skumarravindran\\Desktop\\Cognitive Analytics 101\\Demo\\Twitter Sentiment Analytics")
getwd()
t <- read.csv("Data\\telstra.csv")
head(t)
o <- read.csv("Data\\outage.csv")
head(o)
t <- rbind(t,o)
nrow(t)
Telstr_Tweets <- t$text
length(Telstr_Tweets)
catch.error = function(x)
{
# let us create a missing value for test purpose
y = NA
# try to catch that error (NA) we just created
catch_error = tryCatch(tolower(x), error=function(e) e)
# if not an error
if (!inherits(catch_error, "error"))
y = tolower(x)
# check result if error exists, otherwise the function works fine.
return(y)
}
cleanTweets<- function(tweet){
# Clean the tweet for sentiment analysis
#  remove html links, which are not required for sentiment analysis
tweet = gsub("(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", " ", tweet)
# First we will remove retweet entities from the stored tweets (text)
tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", " ", tweet)
# Then remove all "#Hashtag"
tweet = gsub("#\\w+", " ", tweet)
# Then remove all "@people"
tweet = gsub("@\\w+", " ", tweet)
# Then remove all the punctuation
tweet = gsub("[[:punct:]]", " ", tweet)
# Then remove numbers, we need only text for analytics
tweet = gsub("[[:digit:]]", " ", tweet)
# finally, we remove unnecessary spaces (white spaces, tabs etc)
tweet = gsub("[ \t]{2,}", " ", tweet)
tweet = gsub("^\\s+|\\s+$", "", tweet)
# if anything else, you feel, should be removed, you can. For example "slang words" etc using the above function and methods.
# Next we'll convert all the word in lowar case. This makes uniform pattern.
tweet = catch.error(tweet)
tweet
}
cleanTweetsAndRemoveNAs<- function (Tweets) {
TweetsCleaned = sapply(Tweets, cleanTweets)
# Remove the "na" tweets from this tweet list
TweetsCleaned = TweetsCleaned[!is.na(TweetsCleaned)]
names(TweetsCleaned) = NULL
# Remove the repetitive tweets from this tweet list
TweetsCleaned = unique(TweetsCleaned)
TweetsCleaned
}
Telstr_Tweets_Cleaned = cleanTweetsAndRemoveNAs(Telstr_Tweets)
head(Telstr_Tweets_Cleaned)
write.csv(Telstr_Tweets_Cleaned, "Data\\telstraCleaned.csv")
opinion.lexicon.pos = scan('Data/opinion-lexicon-English/positive-words.txt', what='character', comment.char=';')
opinion.lexicon.neg = scan('Data/opinion-lexicon-English/negative-words.txt', what='character', comment.char=';')
head(opinion.lexicon.neg)
head(opinion.lexicon.pos)
# adding few words to the dictionary
pos.words = c(opinion.lexicon.pos,'upgrade')
neg.words = c(opinion.lexicon.neg,'wait', 'waiting', 'wtf', 'cancellation')
################ score
score.sentiment = function(sentences, pos.words, neg.words, .progress='none')
{
require(plyr)
require(stringr)
# we got a vector of sentences. plyr will handle a list
# or a vector as an "l" for us
# we want a simple array ("a") of scores back, so we use
# "l" + "a" + "ply" = "laply":
scores = laply(sentences, function(sentence, pos.words, neg.words) {
# clean up sentences with R's regex-driven global substitute, gsub():
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl:]]', '', sentence)
sentence = gsub('\\d+', '', sentence)
# and convert to lower case:
sentence = tolower(sentence)
# split into words. str_split is in the stringr package
word.list = str_split(sentence, '\\s+')
# sometimes a list() is one level of hierarchy too much
words = unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
# match() returns the position of the matched term or NA
# we just want a TRUE/FALSE:
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
# and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress=.progress )
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
library(stringr)
library(plyr)
TelstraResult = score.sentiment(Telstr_Tweets_Cleaned, pos.words , neg.words)
head(TelstraResult)
nrow(TelstraResult)
write.csv(TelstraResult, "Data\\scroing.csv")
library(tm)
library(SnowballC)
library(topicmodels)
setwd("C:\\Users\\skumarravindran\\Desktop\\Cognitive Analytics 101\\Demo\\NLP and Topics\\Documents")
#list of the sample text files
filenames <- list.files(getwd(),pattern="*.txt")
# read the text files
files <- lapply(filenames,readLines)
# create corpus from the vector files
docs <- Corpus(VectorSource(files))
#checking a document
writeLines(as.character(docs[[5]]))
# processing the text
docs <-tm_map(docs,content_transformer(tolower))
#removing symbols
toSpace <- content_transformer(function(x, pattern) { return (gsub(pattern, " ", x))})
docs <- tm_map(docs, toSpace, "-")
docs <- tm_map(docs, toSpace, "’")
docs <- tm_map(docs, toSpace, "‘")
docs <- tm_map(docs, toSpace, "•")
docs <- tm_map(docs, toSpace, "”")
docs <- tm_map(docs, toSpace, "“")
#remove punctuation
docs <- tm_map(docs, removePunctuation)
#Strip digits
docs <- tm_map(docs, removeNumbers)
#remove stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
#remove whitespace
docs <- tm_map(docs, stripWhitespace)
#Good practice to check every now and then
writeLines(as.character(docs[[5]]))
#Stem document
docs <- tm_map(docs,stemDocument)
#Create document-term matrix
dtm <- DocumentTermMatrix(docs)
#convert rownames to filenames
rownames(dtm) <- filenames
#Set parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
#Number of topics
k <- 5
#Run LDA using Gibbs sampling
ldaOut <-LDA(dtm,k, method="Gibbs", control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
ldaOut.topics <- as.matrix(topics(ldaOut))
write.csv(ldaOut.topics,file=paste("LDAGibbs",k,"DocsToTopics.csv"))
#top 6 terms in each topic
ldaOut.terms <- as.matrix(terms(ldaOut,6))
write.csv(ldaOut.terms,file=paste("LDAGibbs",k,"TopicsToTerms.csv"))
#probabilities associated with each topic assignment
topicProbabilities <- as.data.frame(ldaOut@gamma)
write.csv(topicProbabilities,file=paste("LDAGibbs",k,"TopicProbabilities.csv"))
setwd("C:\\Users\\skumarravindran\\Desktop\\Cognitive Analytics 101\\Demo\\Recommendation")
getwd()
# reading the dataset
rdata <- read.csv("Data/following.csv")
head(rdata, 10)
library(data.table)
pivoting <- data.table(rdata)
pivotdata<-dcast.data.table(pivoting, Items ~ UserID, fun.aggregate=length, value.var="UserID")
ubs<-read.csv("Data/pivot-follows.csv")
head(ubs)
colnames(ubs)
# Function to calculate the cosine between two vectors
getCosine <- function(x,y)
{
dat <- cbind(x,y)
#f <- as.matrix(dat)
f <- as.data.frame(dat)
# Remove the rows with zeros
datn<- f[-which(rowSums(f==0)>0),]
#colnames(datn)<-c("x","y")
#dat <- as.data.frame(datn)
if(nrow(datn) > 2)
{
this.cosine <- sum(x*y) / (sqrt(sum(x*x)) * sqrt(sum(y*y)))
}
else
{
this.cosine <- 0
}
return(this.cosine)
}
ubs.score  <- matrix(NA, nrow=ncol(ubs),ncol=ncol(ubs),dimnames=list(colnames(ubs),colnames(ubs)))
for(i in 1:ncol(ubs)) {
# Loop through the columns for each column
for(j in 1:ncol(ubs)) {
# Fill in placeholder with cosine similarities
ubs.score[i,j] <- getCosine(data.matrix(ubs[i]),data.matrix(ubs[j]))
}
print(i)
}
ubs.score <- as.data.frame(ubs.score)
head(ubs.score)
user.neighbours <- matrix(NA, nrow=ncol(ubs.score),ncol=11,dimnames=list(colnames(ubs.score)))
for(i in 1:ncol(ubs))
{
# Setting threshold for avoiding zeros
n <- length(ubs.score[,i])
thres <- sort(ubs.score[,i],partial=n-10)[n-10]
if(thres > 0.10)
{
# Choosing the top 10 recommendation
user.neighbours[i,] <- (t(head(n=11,rownames(ubs.score[order(ubs.score[,i],decreasing=TRUE),][i]))))
}
else
{
user.neighbours[i,] <- ""
}
}
head(user.neighbours, 10)
tail(user.neighbours, 20)
allrec <- ""
# getting the item to recommend
for(i in 1:nrow(user.neighbours))
{
# Setting threshold for avoiding zeros
for (j in 2:3)
{
nItem <- user.neighbours[i,j]
rname <- as.data.frame(nItem)
rname <- rownames(rname)
n <- as.numeric(substring(nItem, 2,3))
new <- subset(rdata, UserID == n)
usr <- rname
rec <- cbind(usr,data.frame(new$Items))
allrec <- rbind(allrec,rec)
}
print (i)
}
allrec <- allrec[complete.cases(allrec),]
colnames(allrec) <- c("UserID","Items")
head(allrec, 10)
